---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      chart: loki
      version: 2.13.3
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
        namespace: flux-system
      interval: 5m

  install:
    createNamespace: false
    remediation: # perform remediation when helm install fails
      retries: 3

  upgrade:
    remediation: # perform remediation when helm upgrade fails
      retries: 3
      remediateLastFailure: true # remediate the last failure, when no retries remain
    cleanupOnFail: true

  dependsOn:
    - name: kube-prometheus-stack
      namespace: monitoring

  values:

    image:
      repository: ghcr.io/k8s-at-home/loki

    env:
      - name: TZ
        value: "${TIMEZONE}"

    resources:
      requests:
        cpu: 63m
        memory: 588M
      limits:
        cpu: 63m
        memory: 588M

    probes:
      liveness:
        enabled: false
      readiness:
        enabled: false
      startup:
        enabled: true

    ingress:
      enabled: true
      className: "traefik"
      annotations:
        traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
        external-dns.alpha.kubernetes.io/target: "ipv4.${SECRET_DOMAIN}"
        hajimari.io/enable: "true"
        hajimari.io/icon: "logs"
        hajimari.io/group: "monitoring"
      hosts:
        - host: &host "loki.${SECRET_DOMAIN}"
          paths:
            - /
      tls:
        - hosts:
            - *host

    persistence:
      enabled: true
      existingClaim: loki-storage-pvc

    serviceMonitor:
      enabled: true

    extraArgs:
      log.level: debug

    config:
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/scratch
        alertmanager_url: http://alertmanager-operated.monitoring.svc.cluster.local:9093
        ring:
          kvstore:
            store: inmemory
        enable_api: true

    service:
      type: LoadBalancer # Setting Ip external to cluster for easy port forward
      externalTrafficPolicy: Cluster
      loadBalancerIP: "${SVC_LOKI_ADDR}"

    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} | json | _SYSTEMD_UNIT = "smartmontools.service" !~ "(?i)previous self-test completed without error" !~ "(?i)Prefailure" |~ "(?i)(error|fail)"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "SMART has reported failures on host {{ $labels.hostname }}"
      #
      # zwavejs2mqtt
      #
      - name: zwavejs2mqtt
        rules:
          - alert: ZwaveUnableToReachMQTT
            expr: |
              sum(count_over_time({app="zwavejs2mqtt"} |~ "(?i)error while connecting mqtt"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Zwavejs2mqtt is unable to reach MQTT"
      #
      # zwavejs2mqtt-z-stick-7
      #
      - name: zwavejs2mqtt-z-stick-7
        rules:
          - alert: ZwaveUnableToReachMQTT
            expr: |
              sum(count_over_time({app="zwavejs2mqtt"} |~ "(?i)error while connecting mqtt"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "zwavejs2mqtt-z-stick-7 is unable to reach MQTT"
      #
      # frigate
      #
      - name: frigate
        rules:
          - alert: FrigateUnableToReachMQTT
            expr: |
              sum(count_over_time({app="frigate"} |~ "(?i)unable to connect to mqtt server"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Frigate is unable to reach MQTT"
      #
      # *arr
      #
      - name: arr
        rules:
          - alert: ArrDatabaseIsLocked
            expr: |
              sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database is locked"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "{{ $labels.app }} is experiencing locked database issues"
          - alert: ArrDatabaseIsMalformed
            expr: |
              sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database disk image is malformed"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "{{ $labels.app }} is experiencing malformed database disk image issues"
      #
      # home-assistant
      #
      - name: home-assistant
        rules:
          - alert: HomeAssistantUnableToReachPostgresql
            expr: |
              sum by (app) (count_over_time({app="home-assistant"} |~ "(?i)error in database connectivity"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Home Assistant is unable to connect to postgresql"
      #
      # node-red
      #
      - name: node-red
        rules:
          - alert: NodeRedUnableToReachHomeAssistant
            expr: |
              sum by (app) (count_over_time({app="node-red"} |~ "(?i)home assistant.*connecting to undefined"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Node-Red is unable to connect to Home Assistant"
