---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
  namespace: ai
spec:
  interval: 30m
  chart:
    spec:
      chart: ollama
      version: 0.63.0
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system

  maxHistory: 2

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:
    updateStrategy:
      type: "Recreate"

    replicaCount: 1

    ollama:
      models:
        - codellama
        - mistral
        - llama2
        - llama3.1
        - dolphin-mixtral
        - llama2-uncensored
      gpu:
        # Enable GPU integration
        enabled: true
        # Specify the number of GPU
        number: 1
        # Default model to serve, if not set, no model will be served at container startup
      defaultModel: "codellama"

      serviceAccount:
        # Specifies whether a service account should be created
        create: true
        # Automatically mount a ServiceAccount's API credentials?
        automount: true
        # The name of the service account to use.
        # If not set and create is true, a name is generated using the fullname template
        name: ""

    runtimeClassName: nvidia

    service:
      main:
        ports:
          http:
            port: 11434

    ingress:
      enabled: true
      className: internal
      hosts:
        - host: &host ollama.${SECRET_DOMAIN}
          paths:
            - path: /
              pathType: Prefix

    resources:
      requests:
        memory: 8G
        cpu: 2000m
        nvidia.com/gpu: 1
      limits:
        memory: 8G
        nvidia.com/gpu: 1

    livenessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1
    readinessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 30
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 6
      successThreshold: 1

    persistentVolume:
      enabled: true
      existingClaim: ollama-data-pvc

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                    - "true"

    nodeSelector:
      nvidia.com/gpu.present: "true"
