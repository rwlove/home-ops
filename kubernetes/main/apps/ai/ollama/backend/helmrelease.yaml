---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
  namespace: ai
spec:
  interval: 30m
  chart:
    spec:
      chart: ollama
      version: 1.1.2
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system

  maxHistory: 2

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:
    updateStrategy:
      type: "Recreate"

    replicaCount: 1

    ollama:
      models:
        pull:
          #- llama3.1:8b
          - llama2-uncensored:7b
          - llava-llama3:8b
          - fixt/home-3b-v3:Q5_K_M

        run:
          #- llama3.1:8b          # ??       - OTerm / General
          - llama2-uncensored:7b # 8BG      - OTerm
          - llava-llama3:8b      # ??       - Frigate
          - home-3b-v3:Q5_K_M    # 4.57 GB  - Home Assistant

      gpu:
        # Enable GPU integration
        enabled: true
        # Specify the number of GPU
        number: 1
        # Default model to serve, if not set, no model will be served at container startup
      defaultModel: "llava-llama3:8b"

      serviceAccount:
        # Specifies whether a service account should be created
        create: true
        # Automatically mount a ServiceAccount's API credentials?
        automount: true
        # The name of the service account to use.
        # If not set and create is true, a name is generated using the fullname template
        name: ""

    runtimeClassName: nvidia

    securityContext:
      privileged: true

    service:
      main:
        ports:
          http:
            port: 11434

    ingress:
      enabled: true
      className: internal
      hosts:
        - host: &host ollama.${SECRET_DOMAIN}
          paths:
            - path: /
              pathType: Prefix

    resources:
      requests:
        memory: 8G
        cpu: 2000m
        nvidia.com/gpu: 1
      limits:
        memory: 8G
        nvidia.com/gpu: 1

    livenessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 600
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1

    readinessProbe:
      enabled: true
      path: /
      initialDelaySeconds: 30
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 6
      successThreshold: 1

    persistentVolume:
      enabled: true
      existingClaim: ollama-data-pvc

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                    - "true"

    nodeSelector:
      nvidia.com/gpu.present: "true"
